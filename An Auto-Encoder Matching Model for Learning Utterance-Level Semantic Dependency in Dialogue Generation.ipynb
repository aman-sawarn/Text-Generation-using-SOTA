{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cornell.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is cornell.py file in process folder\n",
    "\n",
    "def cornell_conversations(source_dir: str):\n",
    "    id2line = {}\n",
    "    path = os.path.join(source_dir, _MOVIE_LINES_FILE_NAME)\n",
    "    with open(path, 'r', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = dict(zip(_MOVIE_LINES_FIELDS, line.split(' +++$+++ ')))\n",
    "            text = re.sub('(\\n)|(<u>)|(</u>)|(\\[\\d\\])', '', line['text'])\n",
    "            id2line[line['lineID']] = text\n",
    "\n",
    "    conversations = []\n",
    "    path = os.path.join(source_dir, _MOVIE_CONVERSATIONS_FILE_NAME)\n",
    "    with open(path, 'r', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            conv = dict(zip(_MOVIE_CONVERSATIONS_FIELDS, line.split(' +++$+++ ')))\n",
    "            line_ids = ast.literal_eval(conv['utteranceIDs'])\n",
    "            conversations.append([id2line[i] for i in line_ids])\n",
    "\n",
    "    return conversations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "\n",
    "_MOVIE_LINES_FILE_NAME = r'C:\\Users\\Cyborg\\Documents\\GitHub\\Text-Generation-using-SOTA\\cornell movie-dialogs corpus\\movie_lines.txt'\n",
    "_MOVIE_CONVERSATIONS_FILE_NAME = r'C:\\Users\\Cyborg\\Documents\\GitHub\\Text-Generation-using-SOTA\\cornell movie-dialogs corpus\\movie_conversations.txt'\n",
    "_MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "_MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
       " \"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       " 'Not the hacking and gagging and spitting part.  Please.',\n",
       " \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cornell_conversations(_MOVIE_CONVERSATIONS_FILE_NAME)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "daily.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## daily.py \n",
    "import os\n",
    "\n",
    "_TEXT_FILE_NAME = 'dialogues_text.txt'\n",
    "\n",
    "\n",
    "def daily_conversations(source_dir: str):\n",
    "    path = os.path.join(source_dir, _TEXT_FILE_NAME)\n",
    "    with open(path, 'r') as f:\n",
    "        conversations = [line.split(' __eou__')[:-1] for line in f]\n",
    "        return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_conversations(_TEXT_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import nltk\n",
    "from mlbootstrap.preprocess import BasicPreprocessor\n",
    "import os\n",
    "from process.cornell import cornell_conversations\n",
    "from process.daily import daily_conversations\n",
    "from process.vocab_processor import VocabularyProcessor\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DATA_SPLIT = 0.8\n",
    "\n",
    "def _conversation_to_qa_pairs(conversation: List[str]) -> List[Tuple[str, str]]:\n",
    "    qa_pairs = []\n",
    "    length = len(conversation)\n",
    "    for i in range(length - 1):\n",
    "        q = conversation[i]\n",
    "        a = conversation[i + 1]\n",
    "        qa_pairs.append((q, a))\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def _tokenize(text: str) -> List[List[str]]:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    for sent in sentences:\n",
    "        for i in range(len(sent)):\n",
    "            sent[i] = sent[i].lower()\n",
    "    return sentences\n",
    "\n",
    "def _is_valid_sample(sample: Tuple[List[str], List[str]],\n",
    "                     vocab_processor: VocabularyProcessor) -> bool:\n",
    "    q, a = sample\n",
    "    conditions = [\n",
    "        q,\n",
    "        a,\n",
    "        any(not vocab_processor.is_unknown(w) for w in q),\n",
    "        all(not vocab_processor.is_unknown(w) for w in a)]\n",
    "    return all(conditions)\n",
    "\n",
    "\n",
    "def _vecterize(words: List[str], vocab_processor: VocabularyProcessor) -> List[int]:\n",
    "    return [vocab_processor.word2id(w) for w in words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(BasicPreprocessor):\n",
    "    def finished(self):\n",
    "        dst = self._get_dataset_node().dst\n",
    "        conditions = [\n",
    "            super(DataProcessor, self).finished(),\n",
    "            Path(os.path.join(dst, self.__vocab_filename())).exists(),\n",
    "            Path(os.path.join(dst, self.__dataset_filename())).exists()]\n",
    "        return all(conditions)\n",
    "\n",
    "    def check(self):\n",
    "        super(DataProcessor, self).check()\n",
    "        dst = self._get_dataset_node().dst\n",
    "        if not Path(os.path.join(dst, self.__vocab_filename())).exists():\n",
    "            raise FileNotFoundError(\n",
    "                \"Vocabulary file '{}' does not exist\".format(self.__vocab_filename()))\n",
    "        if not Path(os.path.join(dst, self.__dataset_filename())).exists():\n",
    "            raise FileNotFoundError(\n",
    "                \"Dataset file '{}' does not exist\".format(self.__dataset_filename()))\n",
    "\n",
    "    def _on_next(self, src: str, dst: str, task: str):\n",
    "        os.makedirs(dst, exist_ok=True)\n",
    "\n",
    "        full_set_fp = os.path.join(dst, 'full.dataset')\n",
    "        if not Path(full_set_fp).exists():\n",
    "            fn = {\n",
    "                'cornell': cornell_conversations,\n",
    "                'daily': daily_conversations\n",
    "            }\n",
    "            conversations = fn[task](src)\n",
    "            qa_pairs = [qa for c in conversations for qa in _conversation_to_qa_pairs(c)]\n",
    "            qa_pairs = tqdm(qa_pairs, desc='Tokenizing QA pairs', leave=False)\n",
    "            tokenized_qa_pairs = [(_tokenize(q), _tokenize(a)) for q, a in qa_pairs if q and a]\n",
    "\n",
    "            with open(full_set_fp, 'wb') as f:\n",
    "                pickle.dump(tokenized_qa_pairs, f, -1)\n",
    "            print('Saved full dataset.')\n",
    "        else:\n",
    "            with open(full_set_fp, 'rb') as f:\n",
    "                tokenized_qa_pairs = pickle.load(f)\n",
    "\n",
    "        train = self.__process_train_set(tokenized_qa_pairs, dst)\n",
    "        test = self.__process_test_set(tokenized_qa_pairs, dst)\n",
    "\n",
    "        data = {'train': train, 'test': test}\n",
    "        dataset_filename = self.__dataset_filename()\n",
    "\n",
    "        with open(os.path.join(dst, dataset_filename), 'wb') as f:\n",
    "            pickle.dump(data, f, -1)\n",
    "\n",
    "    def __process_train_set(self, tokenized_qa_pairs, dst: str):\n",
    "        n_samples = len(tokenized_qa_pairs)\n",
    "        train = tokenized_qa_pairs[:round(n_samples * _DATA_SPLIT)]\n",
    "        train = tqdm(train, desc='Flattening QA pairs in training set', leave=False)\n",
    "        train = [\n",
    "            (self.__flatten_tokenized_utterance(q, reverse=True),\n",
    "             self.__flatten_tokenized_utterance(a, reverse=False)) for q, a in train]\n",
    "\n",
    "        min_frequency = self.hyperparameter('min_frequency')\n",
    "        vocab_processor = VocabularyProcessor(min_frequency=min_frequency)\n",
    "        for q, a in tqdm(train, desc='Fitting vocabulary', leave=False):\n",
    "            for word in q + a:\n",
    "                vocab_processor.add(word)\n",
    "        vocab_processor.fit()\n",
    "        vocab_filename = self.__vocab_filename()\n",
    "        vocab_processor.save(os.path.join(dst, vocab_filename))\n",
    "\n",
    "        train = tqdm(train, desc='Vectorizing training samples', leave=False)\n",
    "        train = [(_vecterize(q, vocab_processor), _vecterize(a, vocab_processor)) for q, a in train\n",
    "                 if _is_valid_sample((q, a), vocab_processor)]\n",
    "\n",
    "        return train\n",
    "\n",
    "    def __process_test_set(self, tokenized_qa_pairs, dst: str):\n",
    "        n_samples = len(tokenized_qa_pairs)\n",
    "        test = tokenized_qa_pairs[round(n_samples * _DATA_SPLIT):]\n",
    "        test = tqdm(test, desc='Flattening QA pairs in testing set', leave=False)\n",
    "        test = [\n",
    "            (self.__flatten_tokenized_utterance(q, reverse=True),\n",
    "             self.__flatten_tokenized_utterance(a, reverse=False)) for q, a in test]\n",
    "\n",
    "        vocab_processor = VocabularyProcessor()\n",
    "        vocab_filename = self.__vocab_filename()\n",
    "        vocab_processor.restore(os.path.join(dst, vocab_filename))\n",
    "\n",
    "        test = tqdm(test, desc='Vectorizing test samples', leave=False)\n",
    "        test = [(_vecterize(q, vocab_processor), _vecterize(a, vocab_processor)) for q, a in test\n",
    "                if _is_valid_sample((q, a), vocab_processor)]\n",
    "\n",
    "        return test\n",
    "\n",
    "    def __flatten_tokenized_utterance(self, utterance: List[List[str]], reverse=False) -> List[str]:\n",
    "        flat = []\n",
    "\n",
    "        if reverse:\n",
    "            utterance = reversed(utterance)\n",
    "\n",
    "        for sent in utterance:\n",
    "            max_sent_length = self.hyperparameter('max_sent_length')\n",
    "            if len(flat) + len(sent) <= max_sent_length:\n",
    "                if reverse:\n",
    "                    flat = sent + flat\n",
    "                else:\n",
    "                    flat = flat + sent\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return flat\n",
    "\n",
    "    def __vocab_filename(self) -> str:\n",
    "        max_sent_length = self.hyperparameter('max_sent_length')\n",
    "        min_frequency = self.hyperparameter('min_frequency')\n",
    "        return 'max_sent_length{}-min_frequency{}.vocab'.format(max_sent_length, min_frequency)\n",
    "\n",
    "    def __dataset_filename(self) -> str:\n",
    "        max_sent_length = self.hyperparameter('max_sent_length')\n",
    "        min_frequency = self.hyperparameter('min_frequency')\n",
    "        return 'max_sent_length{}-min_frequency{}.dataset'.format(max_sent_length, min_frequency)\n",
    "\n",
    "    def _load_dataset(self):\n",
    "        dst = self._get_dataset_node().dst\n",
    "        with open(os.path.join(dst, self.__dataset_filename()), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        vocab_processor = VocabularyProcessor()\n",
    "        vocab_processor.restore(os.path.join(dst, self.__vocab_filename()))\n",
    "        data['vocab_processor'] = vocab_processor\n",
    "\n",
    "        # some fixed testing samples in 'test_samples.txt'\n",
    "        with open(os.path.join(dst, 'test_samples.txt'), 'r') as f:\n",
    "            texts = [line[:-1] for line in f]\n",
    "            questions = [_tokenize(text) for text in texts]\n",
    "            questions = [self.__flatten_tokenized_utterance(q, reverse=True) for q in questions]\n",
    "            questions = [_vecterize(q, vocab_processor) for q in questions]\n",
    "        test_samples = [((q, []), text) for q, text in zip(questions, texts) if q]\n",
    "        data['test_samples'] = test_samples\n",
    "\n",
    "        print('Loaded dataset: {} words, {} training samples, {} testing samples'.format(\n",
    "            vocab_processor.size(), len(data['train']), len(data['test'])))\n",
    "\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _conversation_to_qa_pairs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab_processor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "GO_TOKEN = '<go>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNKNOWN_TOKEN = '<unk>'\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    GO_TOKEN, EOS_TOKEN, PAD_TOKEN, UNKNOWN_TOKEN\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyProcessor:\n",
    "    def __init__(self, min_frequency: int = 0):\n",
    "        self._word2id = {}\n",
    "        self._id2word = {}\n",
    "        self._word_frequency = {}\n",
    "        self._min_frequency = min_frequency\n",
    "\n",
    "        self.add(GO_TOKEN)\n",
    "        self.add(EOS_TOKEN)\n",
    "        self.add(PAD_TOKEN)\n",
    "        self.add(UNKNOWN_TOKEN)\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self._word2id)\n",
    "\n",
    "    def add(self, word: str, frequency: int = 1):\n",
    "        word = word.lower()\n",
    "        if word in self._word2id:\n",
    "            self._word_frequency[word] += frequency\n",
    "        else:\n",
    "            word_id = len(self._word2id)\n",
    "            self._word2id[word] = word_id\n",
    "            self._id2word[word_id] = word\n",
    "            self._word_frequency[word] = frequency\n",
    "\n",
    "    def fit(self):\n",
    "        vocab_processor = VocabularyProcessor()\n",
    "        for word, frequency in self._word_frequency.items():\n",
    "            if word not in SPECIAL_TOKENS and frequency >= self._min_frequency:\n",
    "                vocab_processor.add(word, frequency)\n",
    "\n",
    "        min_frequency = self._min_frequency\n",
    "        self.__dict__.update(vocab_processor.__dict__)\n",
    "        self._min_frequency = min_frequency\n",
    "\n",
    "    def save(self, filename: str):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f, -1)\n",
    "\n",
    "    def restore(self, filename: str):\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.__dict__.update(pickle.load(f))\n",
    "\n",
    "    def word2id(self, word: str) -> int:\n",
    "        word = word.lower()\n",
    "        return self._word2id.get(word, self._word2id[UNKNOWN_TOKEN])\n",
    "\n",
    "    def id2word(self, word_id: int) -> str:\n",
    "        return self._id2word[word_id]\n",
    "\n",
    "    def word_frequency(self, word: str) -> int:\n",
    "        word = word.lower()\n",
    "        return self._word_frequency[word] if word in self._word2id else 0\n",
    "\n",
    "    def is_unknown(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        return word not in self._word2id or word == UNKNOWN_TOKEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auto_encoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from model.seq import Seq2SeqModel\n",
    "from model.tf_rnn_helper import *\n",
    "from model.batch import Batch\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderModel(Seq2SeqModel):\n",
    "    def __init__(self, name: str = 'auto'):\n",
    "        super(AutoEncoderModel, self).__init__()\n",
    "\n",
    "        self.q_dec = None\n",
    "        self.q_target = None\n",
    "        self.q_weights = None\n",
    "        self.a_enc = None\n",
    "\n",
    "        self.q_dec_outputs = None\n",
    "        self.a_dec_outputs = None\n",
    "\n",
    "        self.q_loss_op = None\n",
    "        self.q_train_op = None\n",
    "        self.a_loss_op = None\n",
    "        self.a_train_op = None\n",
    "\n",
    "    def _build_placeholders(self):\n",
    "        super(AutoEncoderModel, self)._build_placeholders()\n",
    "\n",
    "        with tf.name_scope('placeholder'):\n",
    "            with tf.name_scope('q'):\n",
    "                self.q_dec = [tf.placeholder(tf.int32, [None, ], name='decoders') for _ in\n",
    "                              range(self.dec_length)]\n",
    "                self.q_target = [tf.placeholder(tf.int32, [None, ], name='target') for _ in\n",
    "                                 range(self.dec_length)]\n",
    "                self.q_weights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in\n",
    "                                  range(self.dec_length)]\n",
    "            with tf.name_scope('a'):\n",
    "                self.a_enc = [tf.placeholder(tf.int32, [None, ], name='encoders') for _ in\n",
    "                              range(self.enc_length)]\n",
    "\n",
    "    def _build_seq2seq(self, mode: str):\n",
    "        super(AutoEncoderModel, self)._build_seq2seq(mode)\n",
    "\n",
    "        with tf.name_scope('seq2seq'):\n",
    "            n_layers = self.hyperparameter('n_layers')\n",
    "\n",
    "            q_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                [self._create_rnn_cell(mode) for _ in range(n_layers)])\n",
    "            a_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                [self._create_rnn_cell(mode) for _ in range(n_layers)])\n",
    "\n",
    "            embedding_size = self.hyperparameter('embedding_size')\n",
    "            output_projection = self.output_projection.variables() if \\\n",
    "                self.output_projection else None\n",
    "\n",
    "            q_states, q_attn_states = embedding_rnn_encoder(\n",
    "                self.q_enc,\n",
    "                q_cell,\n",
    "                self._vocab_size(),\n",
    "                embedding_size,\n",
    "                scope='q_encoder')\n",
    "            self.q_dec_outputs, _ = embedding_rnn_decoder(\n",
    "                self.q_dec,\n",
    "                q_states,\n",
    "                q_cell,\n",
    "                self._vocab_size(),\n",
    "                embedding_size,\n",
    "                output_projection=output_projection,\n",
    "                feed_previous=mode == 'test',\n",
    "                scope='q_decoder')\n",
    "\n",
    "            a_states, _ = embedding_rnn_encoder(\n",
    "                self.a_enc,\n",
    "                a_cell,\n",
    "                self._vocab_size(),\n",
    "                embedding_size,\n",
    "                scope='a_encoder')\n",
    "            self.a_dec_outputs, _ = embedding_rnn_decoder(\n",
    "                self.a_dec,\n",
    "                a_states,\n",
    "                a_cell,\n",
    "                self._vocab_size(),\n",
    "                embedding_size,\n",
    "                output_projection=output_projection,\n",
    "                feed_previous=mode == 'test',\n",
    "                scope='a_decoder')\n",
    "\n",
    "            states_p = states_projection(\n",
    "                q_states,\n",
    "                self.hyperparameter('hidden_size'),\n",
    "                activate_fn=tf.nn.tanh,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            if 'attn' in self.name:  # with attention\n",
    "                self.seq_dec_outputs, _ = embedding_attention_decoder(\n",
    "                    self.a_dec,\n",
    "                    states_p,\n",
    "                    q_attn_states,\n",
    "                    a_cell,\n",
    "                    self._vocab_size(),\n",
    "                    embedding_size,\n",
    "                    output_projection=output_projection,\n",
    "                    feed_previous=mode == 'test',\n",
    "                    scope='seq_decoder')\n",
    "            else:\n",
    "                self.seq_dec_outputs, _ = embedding_rnn_decoder(\n",
    "                    self.a_dec,\n",
    "                    states_p,\n",
    "                    a_cell,\n",
    "                    self._vocab_size(),\n",
    "                    embedding_size,\n",
    "                    output_projection=output_projection,\n",
    "                    feed_previous=mode == 'test',\n",
    "                    scope='seq_decoder')\n",
    "\n",
    "    def _build_optimize_ops(self):\n",
    "        super(AutoEncoderModel, self)._build_optimize_ops()\n",
    "\n",
    "        self.q_loss_op = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "            self.q_dec_outputs,\n",
    "            self.q_target,\n",
    "            self.q_weights,\n",
    "            self._vocab_size(),\n",
    "            softmax_loss_function=self._sampled_softmax_fn if self.output_projection else None)\n",
    "        self.a_loss_op = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "            self.a_dec_outputs,\n",
    "            self.a_target,\n",
    "            self.a_weights,\n",
    "            self._vocab_size(),\n",
    "            softmax_loss_function=self._sampled_softmax_fn if self.output_projection else None)\n",
    "\n",
    "        self.q_train_op = self.optimizer.minimize(self.q_loss_op)\n",
    "        self.a_train_op = self.optimizer.minimize(self.a_loss_op)\n",
    "\n",
    "    def _train_step(self, batch: Batch):\n",
    "        feed_dict = {}\n",
    "        for i in range(self.enc_length):\n",
    "            feed_dict[self.q_enc[i]] = batch.q_enc_seq[i]\n",
    "            feed_dict[self.a_enc[i]] = batch.a_enc_seq[i]\n",
    "        for i in range(self.dec_length):\n",
    "            feed_dict[self.q_dec[i]] = batch.q_dec_seq[i]\n",
    "            feed_dict[self.q_target[i]] = batch.q_target_seq[i]\n",
    "            feed_dict[self.q_weights[i]] = batch.q_weights[i]\n",
    "            feed_dict[self.a_dec[i]] = batch.a_dec_seq[i]\n",
    "            feed_dict[self.a_target[i]] = batch.a_target_seq[i]\n",
    "            feed_dict[self.a_weights[i]] = batch.a_weights[i]\n",
    "\n",
    "        _, q_loss, _, a_loss = self.sess.run(\n",
    "            [self.q_train_op, self.q_loss_op, self.a_train_op, self.a_loss_op],\n",
    "            feed_dict=feed_dict)\n",
    "\n",
    "        _, loss = self.sess.run([self.seq_train_op, self.seq_loss_op], feed_dict=feed_dict)\n",
    "\n",
    "        # Print training status\n",
    "        if self.global_step % self.training_parameter('print_interval') == 0:\n",
    "            perplexity = math.exp(float(loss) if loss < 300 else float('inf'))\n",
    "            tqdm.write(\n",
    "                '----- Step %d -- Q Loss %.2f -- A Loss %.2f -- Loss %.2f -- Perplexity %.2f' % (\n",
    "                    self.global_step, q_loss, a_loss, loss, perplexity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlbootstrap.model import BasicModel\n",
    "from model.batch import Batch\n",
    "from typing import List, Tuple\n",
    "from process.vocab_processor import VocabularyProcessor, GO_TOKEN, EOS_TOKEN, PAD_TOKEN\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODEL_STATUS_FILENAME = 'model_status.yaml'\n",
    "\n",
    "\n",
    "class BasicChatbotModel(BasicModel):\n",
    "    def __init__(self, name: str = 'basic_chatbot_model'):\n",
    "        super(BasicChatbotModel, self).__init__()\n",
    "\n",
    "        self.enc_length = None\n",
    "        self.dec_length = None\n",
    "\n",
    "        self.sess: tf.Session = None\n",
    "        self.saver: tf.train.Saver = None\n",
    "\n",
    "        self.global_step = 0\n",
    "\n",
    "    def train(self):\n",
    "        self._restore_model_settings()\n",
    "        self._build_graph('train')\n",
    "        self._create_session()\n",
    "        self._restore_checkpoint()\n",
    "\n",
    "        print('Start training ...')\n",
    "        epoch = self.training_parameter('epoch')\n",
    "\n",
    "        for e in range(1, epoch + 1):\n",
    "            print()\n",
    "            learning_rate = self.training_parameter('learning_rate')\n",
    "            print(\n",
    "                '----- Epoch {}/{} ; (learning_rate={}) -----'.format(e, epoch, learning_rate))\n",
    "\n",
    "            tic = datetime.datetime.now()\n",
    "            batches = self._get_batches('train')\n",
    "            for batch in tqdm(batches, desc='Training'):\n",
    "                self.global_step += 1\n",
    "                self._train_step(batch)\n",
    "\n",
    "                if self.global_step % self.training_parameter('save_interval') == 0:\n",
    "                    self._save_checkpoint()\n",
    "\n",
    "            toc = datetime.datetime.now()\n",
    "            print('Epoch finished in {}'.format(toc - tic))\n",
    "\n",
    "    def _restore_model_settings(self):\n",
    "        path = os.path.join(self._config['model']['save_path'], self.name)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        status_path = os.path.join(path, _MODEL_STATUS_FILENAME)\n",
    "\n",
    "        if os.path.exists(status_path):\n",
    "            with open(status_path, 'r') as stream:\n",
    "                model_status = yaml.load(stream)\n",
    "                self.global_step = model_status['global_step']\n",
    "                self._config['hyperparameter'] = model_status['hyperparameter']\n",
    "\n",
    "        max_sent_length = self.hyperparameter('max_sent_length')\n",
    "        self.enc_length = max_sent_length\n",
    "        self.dec_length = max_sent_length + 2\n",
    "\n",
    "    def _build_graph(self, mode: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _create_session(self):\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            gpu_options=tf.GPUOptions(allow_growth=True)\n",
    "        ))\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def _restore_checkpoint(self):\n",
    "        path = os.path.join(self._config['model']['save_path'], self.name)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        status_path = os.path.join(path, _MODEL_STATUS_FILENAME)\n",
    "\n",
    "        if os.path.exists(status_path):\n",
    "            ckpt = tf.train.get_checkpoint_state(path)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "            else:\n",
    "                print('No checkpoint found')\n",
    "                exit(1)\n",
    "        else:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        tqdm.write(\"Checkpoint reached: saving model (don't stop the run) ...\")\n",
    "\n",
    "        path = os.path.join(self._config['model']['save_path'], self.name)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        self._save_model_status(path)\n",
    "\n",
    "        model_path = os.path.join(path, 'model')\n",
    "        self.saver.save(self.sess, model_path, global_step=self.global_step)\n",
    "        tqdm.write('Model saved.')\n",
    "\n",
    "    def _save_model_status(self, path: str):\n",
    "        model_status = {\n",
    "            'hyperparameter': self._config['hyperparameter'],\n",
    "            'global_step': self.global_step}\n",
    "        status_path = os.path.join(path, _MODEL_STATUS_FILENAME)\n",
    "        with open(status_path, 'w') as stream:\n",
    "            yaml.dump(model_status, stream)\n",
    "\n",
    "    def _train_step(self, batch: Batch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _vocab_size(self):\n",
    "        return self.dataset['vocab_processor'].size()\n",
    "\n",
    "    def _get_batches(self, mode: str) -> List[Batch]:\n",
    "        if mode == 'train':\n",
    "            random.shuffle(self.dataset[mode])\n",
    "        samples = self.dataset[mode]\n",
    "        batch_size = self.training_parameter('batch_size')\n",
    "        samples_list = [samples[i:min(i + batch_size, len(samples))] for i in\n",
    "                        range(0, len(samples), batch_size)]\n",
    "        return [self._create_batch(samples) for samples in samples_list]\n",
    "\n",
    "    def _create_batch(self, samples: List[Tuple[List, List]]) -> Batch:\n",
    "        batch = Batch()\n",
    "        vocab_processor: VocabularyProcessor = self.dataset['vocab_processor']\n",
    "        go_id = vocab_processor.word2id(GO_TOKEN)\n",
    "        eos_id = vocab_processor.word2id(EOS_TOKEN)\n",
    "        pad_id = vocab_processor.word2id(PAD_TOKEN)\n",
    "\n",
    "        for q, a in samples:\n",
    "            def __pad(arr, pad_token, length, from_left=False):\n",
    "                if len(arr) >= length:\n",
    "                    return arr\n",
    "                padding = [pad_token] * (length - len(arr))\n",
    "                if from_left:\n",
    "                    arr = padding + arr\n",
    "                else:\n",
    "                    arr = arr + padding\n",
    "                return arr\n",
    "\n",
    "            q_enc_seq = __pad(list(reversed(q)), pad_id, self.enc_length, from_left=True)\n",
    "            batch.q_enc_seq.append(q_enc_seq)\n",
    "\n",
    "            q_dec_seq = __pad([go_id] + q + [eos_id], pad_id, self.dec_length)\n",
    "            batch.q_dec_seq.append(q_dec_seq)\n",
    "\n",
    "            q_target_seq = __pad(q + [eos_id], pad_id, self.dec_length)\n",
    "            batch.q_target_seq.append(q_target_seq)\n",
    "\n",
    "            q_weights = __pad([1.0] * len(q), 0.0, self.dec_length)\n",
    "            batch.q_weights.append(q_weights)\n",
    "\n",
    "            a_enc_seq = __pad(list(reversed(a)), pad_id, self.enc_length, from_left=True)\n",
    "            batch.a_enc_seq.append(a_enc_seq)\n",
    "\n",
    "            a_dec_seq = __pad([go_id] + a + [eos_id], pad_id, self.dec_length)\n",
    "            batch.a_dec_seq.append(a_dec_seq)\n",
    "\n",
    "            a_target_seq = __pad(a + [eos_id], pad_id, self.dec_length)\n",
    "            batch.a_target_seq.append(a_target_seq)\n",
    "\n",
    "            a_weights = __pad([1.0] * len(a), 0.0, self.dec_length)\n",
    "            batch.a_weights.append(a_weights)\n",
    "\n",
    "        def __transpose(arr):\n",
    "            len1 = len(arr)\n",
    "            len2 = len(arr[0])\n",
    "            result = []\n",
    "            for _i in range(len2):\n",
    "                next_arr = []\n",
    "                for _j in range(len1):\n",
    "                    next_arr.append(arr[_j][_i])\n",
    "                result.append(next_arr)\n",
    "            return result\n",
    "\n",
    "        batch.q_enc_seq = __transpose(batch.q_enc_seq)\n",
    "        batch.q_dec_seq = __transpose(batch.q_dec_seq)\n",
    "        batch.q_target_seq = __transpose(batch.q_target_seq)\n",
    "        batch.q_weights = __transpose(batch.q_weights)\n",
    "        batch.a_enc_seq = __transpose(batch.a_enc_seq)\n",
    "        batch.a_dec_seq = __transpose(batch.a_dec_seq)\n",
    "        batch.a_target_seq = __transpose(batch.a_target_seq)\n",
    "        batch.a_weights = __transpose(batch.a_weights)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self):\n",
    "        self.q_enc_seq = []\n",
    "        self.q_dec_seq = []\n",
    "        self.q_target_seq = []\n",
    "        self.q_weights = []\n",
    "        self.a_enc_seq = []\n",
    "        self.a_dec_seq = []\n",
    "        self.a_target_seq = []\n",
    "        self.a_weights = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.base import BasicChatbotModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from model.batch import Batch\n",
    "from typing import List\n",
    "from process.vocab_processor import VocabularyProcessor, GO_TOKEN, EOS_TOKEN, PAD_TOKEN\n",
    "import string\n",
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection:\n",
    "    def __init__(self, shape: List[int], scope=None, dtype=None):\n",
    "        self.scope = scope\n",
    "        assert len(shape) == 2\n",
    "        with tf.variable_scope(scope, dtype=dtype):\n",
    "            self.W = tf.get_variable('weight', shape)\n",
    "            self.b = tf.get_variable('bias', shape[1:], initializer=tf.constant_initializer())\n",
    "\n",
    "    def variables(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    def __call__(self, x):\n",
    "        with tf.name_scope(self.scope):\n",
    "            return x @ self.W + self.b\n",
    "\n",
    "\n",
    "class Seq2SeqModel(BasicChatbotModel):\n",
    "    def __init__(self, name: str = 'seq'):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.q_enc = None\n",
    "        self.a_dec = None\n",
    "        self.a_target = None\n",
    "        self.a_weights = None\n",
    "\n",
    "        self.seq_dec_outputs = None\n",
    "\n",
    "        self.output_projection: Projection = None\n",
    "\n",
    "        self.outputs = None\n",
    "\n",
    "        self.optimizer = None\n",
    "\n",
    "        self.seq_loss_op = None\n",
    "        self.seq_train_op = None\n",
    "\n",
    "    def _build_graph(self, mode: str):\n",
    "        print('Building graph ...')\n",
    "        self._build_placeholders()\n",
    "        self._build_seq2seq(mode)\n",
    "        if mode == 'train':\n",
    "            self._build_optimize_ops()\n",
    "        else:\n",
    "            self._build_outputs_op()\n",
    "\n",
    "    def _build_placeholders(self):\n",
    "        with tf.name_scope('placeholder'):\n",
    "            with tf.name_scope('q'):\n",
    "                self.q_enc = [tf.placeholder(tf.int32, [None, ], name='encoders') for _ in\n",
    "                              range(self.enc_length)]\n",
    "            with tf.name_scope('a'):\n",
    "                self.a_dec = [tf.placeholder(tf.int32, [None, ], name='decoders') for _ in\n",
    "                              range(self.dec_length)]\n",
    "                self.a_target = [tf.placeholder(tf.int32, [None, ], name='target') for _ in\n",
    "                                 range(self.dec_length)]\n",
    "                self.a_weights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in\n",
    "                                  range(self.dec_length)]\n",
    "\n",
    "    def _build_seq2seq(self, mode: str):\n",
    "        with tf.name_scope('seq2seq'):\n",
    "            if 0 < self.hyperparameter('n_sampled') < self._vocab_size():\n",
    "                vocab_size = self._vocab_size()\n",
    "                hidden_size = self.hyperparameter('hidden_size')\n",
    "                self.output_projection = Projection(\n",
    "                    [hidden_size, vocab_size], scope='output_projection', dtype=tf.float32)\n",
    "\n",
    "            n_layers = self.hyperparameter('n_layers')\n",
    "            cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                [self._create_rnn_cell(mode) for _ in range(n_layers)])\n",
    "\n",
    "            embedding_size = self.hyperparameter('embedding_size')\n",
    "            output_projection = self.output_projection.variables() if \\\n",
    "                self.output_projection else None\n",
    "\n",
    "            self.seq_dec_outputs, _ = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "                self.q_enc,\n",
    "                self.a_dec,\n",
    "                cell,\n",
    "                self._vocab_size(),\n",
    "                self._vocab_size(),\n",
    "                embedding_size=embedding_size,\n",
    "                output_projection=output_projection,\n",
    "                feed_previous=mode == 'test')\n",
    "\n",
    "    def _create_rnn_cell(self, mode: str):\n",
    "        hidden_size = self.hyperparameter('hidden_size')\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "\n",
    "        if mode == 'train':\n",
    "            dropout_keep_prob = self.hyperparameter('dropout')\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell,\n",
    "                input_keep_prob=1.0,\n",
    "                output_keep_prob=dropout_keep_prob)\n",
    "\n",
    "        return cell\n",
    "\n",
    "    def _build_optimize_ops(self):\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.training_parameter('learning_rate'))\n",
    "\n",
    "        self.seq_loss_op = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "            self.seq_dec_outputs,\n",
    "            self.a_target,\n",
    "            self.a_weights,\n",
    "            self._vocab_size(),\n",
    "            softmax_loss_function=self._sampled_softmax_fn if self.output_projection else None\n",
    "        )\n",
    "        self.seq_train_op = self.optimizer.minimize(self.seq_loss_op)\n",
    "\n",
    "    def _build_outputs_op(self):\n",
    "        if not self.output_projection:\n",
    "            self.outputs = self.seq_dec_outputs\n",
    "        else:\n",
    "            self.outputs = [self.output_projection(o) for o in self.seq_dec_outputs]\n",
    "\n",
    "        self.outputs = [tf.argmax(o, axis=1) for o in self.outputs]\n",
    "\n",
    "    def _sampled_softmax_fn(self, labels, logits):\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        W_t = tf.transpose(self.output_projection.W)\n",
    "        logits = tf.cast(logits, tf.float32)\n",
    "\n",
    "        return tf.nn.sampled_softmax_loss(\n",
    "            W_t,\n",
    "            self.output_projection.b,\n",
    "            labels,\n",
    "            logits,\n",
    "            self.hyperparameter('n_sampled'),\n",
    "            self._vocab_size())\n",
    "\n",
    "    def _train_step(self, batch: Batch):\n",
    "        feed_dict = {}\n",
    "        for i in range(self.enc_length):\n",
    "            feed_dict[self.q_enc[i]] = batch.q_enc_seq[i]\n",
    "        for i in range(self.dec_length):\n",
    "            feed_dict[self.a_dec[i]] = batch.a_dec_seq[i]\n",
    "            feed_dict[self.a_target[i]] = batch.a_target_seq[i]\n",
    "            feed_dict[self.a_weights[i]] = batch.a_weights[i]\n",
    "\n",
    "        _, loss = self.sess.run([self.seq_train_op, self.seq_loss_op], feed_dict=feed_dict)\n",
    "\n",
    "        # Print training status\n",
    "        if self.global_step % self.training_parameter('print_interval') == 0:\n",
    "            perplexity = math.exp(float(loss) if loss < 300 else float('inf'))\n",
    "            tqdm.write('----- Step %d -- Loss %.2f -- Perplexity %.2f' % (\n",
    "                self.global_step, loss, perplexity))\n",
    "\n",
    "    def evaluate(self):\n",
    "        self._restore_model_settings()\n",
    "        self._build_graph('test')\n",
    "        self._create_session()\n",
    "        self._restore_checkpoint()\n",
    "\n",
    "        print('Start testing ...')\n",
    "        batches = self._get_batches('test')\n",
    "        test_samples = [qa for qa, _ in self.dataset['test_samples']]\n",
    "        test_samples_text = [text for _, text in self.dataset['test_samples']]\n",
    "        # batches = [self._create_batch(test_samples)]\n",
    "        all_inputs = []\n",
    "        all_outputs = []\n",
    "        all_references = []\n",
    "\n",
    "        for batch in tqdm(batches, desc='Testing'):\n",
    "            feed_dict = {}\n",
    "            for i in range(self.enc_length):\n",
    "                feed_dict[self.q_enc[i]] = batch.q_enc_seq[i]\n",
    "            feed_dict[self.a_dec[0]] = batch.a_dec_seq[0]\n",
    "\n",
    "            [outputs] = self.sess.run([self.outputs], feed_dict=feed_dict)\n",
    "            all_outputs += np.transpose(np.array(outputs)).tolist()\n",
    "            all_inputs += np.transpose(np.array(batch.q_enc_seq)).tolist()\n",
    "            all_references += np.transpose(np.array(batch.a_target_seq)).tolist()\n",
    "\n",
    "        # self._write_test_samples_literal(test_samples_text, all_outputs)\n",
    "        # self._write_test_samples_results(all_outputs)\n",
    "        # self._wirte_test_literal(all_inputs, all_outputs)\n",
    "        self._write_evaluation_results(all_outputs, all_references)\n",
    "\n",
    "    def _write_test_samples_results(self, outputs: List[List[int]]):\n",
    "        results = {}\n",
    "        outputs = [self._ids2tokens(tokens) for tokens in outputs]\n",
    "\n",
    "        grams = {1: set(), 2: set(), 3: set()}\n",
    "        for g in grams:\n",
    "            for tokens in outputs:\n",
    "                for i in range(len(tokens)):\n",
    "                    if i + g >= len(tokens):\n",
    "                        break\n",
    "                    grams[g].add(tuple(tokens[i:i + g]))\n",
    "        results.update(dict(('{}-gram'.format(n), len(v)) for n, v in grams.items()))\n",
    "\n",
    "        path = os.path.join(self._config['model']['save_path'], self.name)\n",
    "        result_path = os.path.join(path, 'test_samples_results.json')\n",
    "        with open(result_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2, sort_keys=True)\n",
    "\n",
    "    def _write_evaluation_results(self, outputs: List[List[int]], references: List[List[int]] = None):\n",
    "        results = {}\n",
    "        outputs = [self._ids2tokens(tokens) for tokens in outputs]\n",
    "        references = [self._ids2tokens(tokens) for tokens in references]\n",
    "\n",
    "        weights = [\n",
    "            (1, 0, 0, 0),\n",
    "            (0.5, 0.5, 0, 0),\n",
    "            (0.33, 0.33, 0.33, 0),\n",
    "            (0.25, 0.25, 0.25, 0.25)\n",
    "        ]\n",
    "\n",
    "        smoothing_fn = SmoothingFunction().method1\n",
    "        pairs = tqdm([i for i in zip(outputs, references)], desc='Computing BLEU score')\n",
    "        scores = [np.average(\n",
    "            [sentence_bleu([ref], pred, weights=w, smoothing_function=smoothing_fn) for pred, ref in pairs]) for w in weights]\n",
    "        for i, score in enumerate(scores):\n",
    "            results['BLEU-{}'.format(i + 1)] = score\n",
    "\n",
    "        grams = {1: set(), 2: set(), 3: set()}\n",
    "        for g in grams:\n",
    "            for tokens in outputs:\n",
    "                for i in range(len(tokens)):\n",
    "                    if i + g >= len(tokens):\n",
    "                        break\n",
    "                    grams[g].add(tuple(tokens[i:i + g]))\n",
    "        results.update(dict(('{}-gram'.format(n), len(v)) for n, v in grams.items()))\n",
    "\n",
    "        path = os.path.join(self._config['model']['save_path'], self.name)\n",
    "        result_path = os.path.join(path, 'evaluation_results.json')\n",
    "        with open(result_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2, sort_keys=True)\n",
    "\n",
    "    def _wirte_test_literal(self, inputs: List[List[int]], outputs: List[List[int]]):\n",
    "        inputs = [self._ids2tokens(tokens, reverse=True) for tokens in inputs]\n",
    "        outputs = [self._ids2tokens(tokens) for tokens in outputs]\n",
    "        inputs = [self._tokens2literal(sent) for sent in inputs]\n",
    "        outputs = [self._tokens2literal(sent) for sent in outputs]\n",
    "\n",
    "        path = os.path.join(self._config['model']['save_path'], self.name)\n",
    "        test_literal_path = os.path.join(path, 'test_samples.txt')\n",
    "        with open(test_literal_path, 'w') as f:\n",
    "            for q, a in zip(inputs, outputs):\n",
    "                f.write(q + ' +++$+++ ' + a + '\\n')\n",
    "\n",
    "    def _write_test_samples_literal(self, text: List[str], outputs: List[List[int]]):\n",
    "        outputs = [self._ids2tokens(tokens) for tokens in outputs]\n",
    "        outputs = [self._tokens2literal(sent) for sent in outputs]\n",
    "\n",
    "        path = os.path.join(self._config['model']['save_path'], self.name)\n",
    "        test_literal_path = os.path.join(path, 'example_questions.txt')\n",
    "        with open(test_literal_path, 'w') as f:\n",
    "            for q, a in zip(text, outputs):\n",
    "                f.write(q + ' +++$+++ ' + a + '\\n')\n",
    "\n",
    "    def _ids2tokens(self, seq: List[int], reverse=False):\n",
    "        if not seq:\n",
    "            return ''\n",
    "\n",
    "        if reverse:\n",
    "            seq = reversed(seq)\n",
    "\n",
    "        vocab_processor: VocabularyProcessor = self.dataset['vocab_processor']\n",
    "        sent = []\n",
    "        for word_id in seq:\n",
    "            word = vocab_processor.id2word(word_id)\n",
    "            if word == EOS_TOKEN:\n",
    "                break\n",
    "            elif word not in [GO_TOKEN, PAD_TOKEN]:\n",
    "                sent.append(word)\n",
    "\n",
    "        return sent\n",
    "\n",
    "    @staticmethod\n",
    "    def _tokens2literal(sent: List[str]):\n",
    "        text = ''.join(\n",
    "            [' ' + t if not t.startswith(\"'\") and t not in string.punctuation else t for t in sent])\n",
    "        return text.strip().capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq_attn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.seq import Seq2SeqModel\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionModel(Seq2SeqModel):\n",
    "    def __init__(self, name: str = 'seq-attn'):\n",
    "        super(Seq2SeqAttentionModel, self).__init__(name)\n",
    "\n",
    "    def _build_seq2seq(self, mode: str):\n",
    "        super(Seq2SeqAttentionModel, self)._build_seq2seq(mode)\n",
    "\n",
    "        with tf.name_scope('seq2seq'):\n",
    "            n_layers = self.hyperparameter('n_layers')\n",
    "            cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                [self._create_rnn_cell(mode) for _ in range(n_layers)])\n",
    "\n",
    "            embedding_size = self.hyperparameter('embedding_size')\n",
    "            output_projection = self.output_projection.variables() if \\\n",
    "                self.output_projection else None\n",
    "\n",
    "            self.seq_dec_outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                self.q_enc,\n",
    "                self.a_dec,\n",
    "                cell,\n",
    "                self._vocab_size(),\n",
    "                self._vocab_size(),\n",
    "                embedding_size,\n",
    "                output_projection=output_projection,\n",
    "                feed_previous=mode == 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf_rnn_helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn.python.ops import core_rnn_cell\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.contrib.legacy_seq2seq.python.ops.seq2seq import \\\n",
    "    embedding_rnn_decoder as tf_embedding_rnn_decoder\n",
    "from tensorflow.contrib.legacy_seq2seq.python.ops.seq2seq import \\\n",
    "    embedding_attention_decoder as tf_embedding_attention_decoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "from tensorflow.python.ops import array_ops\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_rnn_encoder(encoder_inputs,\n",
    "                          cell,\n",
    "                          num_symbols,\n",
    "                          embedding_size,\n",
    "                          scope=None,\n",
    "                          dtype=None):\n",
    "    with variable_scope.variable_scope(scope or \"embedding_rnn_encoder\", dtype=dtype) as scope:\n",
    "        dtype = scope.dtype\n",
    "\n",
    "        # Note that we use a deep copy of the original cell\n",
    "        encoder_cell = copy.deepcopy(cell)\n",
    "        encoder_cell = core_rnn_cell.EmbeddingWrapper(\n",
    "            encoder_cell,\n",
    "            embedding_classes=num_symbols,\n",
    "            embedding_size=embedding_size)\n",
    "        encoder_outputs, encoder_state = rnn.static_rnn(\n",
    "            encoder_cell, encoder_inputs, dtype=dtype)\n",
    "\n",
    "        top_states = [array_ops.reshape(e, [-1, 1, cell.output_size]) for e in encoder_outputs]\n",
    "        attention_states = array_ops.concat(top_states, 1)\n",
    "\n",
    "        return encoder_state, attention_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_rnn_decoder(decoder_inputs,\n",
    "                          initial_state,\n",
    "                          cell,\n",
    "                          num_symbols,\n",
    "                          embedding_size,\n",
    "                          output_projection=None,\n",
    "                          feed_previous=False,\n",
    "                          scope=None):\n",
    "    with variable_scope.variable_scope(scope or \"embedding_rnn_decoder\"):\n",
    "        # Node that we use the original cell\n",
    "        if output_projection is None:\n",
    "            cell = core_rnn_cell.OutputProjectionWrapper(cell, num_symbols)\n",
    "\n",
    "        return tf_embedding_rnn_decoder(\n",
    "            decoder_inputs,\n",
    "            initial_state,\n",
    "            cell,\n",
    "            num_symbols,\n",
    "            embedding_size,\n",
    "            output_projection=output_projection,\n",
    "            feed_previous=feed_previous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_attention_decoder(decoder_inputs,\n",
    "                                initial_state,\n",
    "                                attention_states,\n",
    "                                cell,\n",
    "                                num_symbols,\n",
    "                                embedding_size,\n",
    "                                num_heads=1,\n",
    "                                output_size=None,\n",
    "                                output_projection=None,\n",
    "                                feed_previous=False,\n",
    "                                scope=None,\n",
    "                                initial_state_attention=False):\n",
    "    with variable_scope.variable_scope(scope or \"embedding_attention_decoder\"):\n",
    "        if output_projection is None:\n",
    "            cell = core_rnn_cell.OutputProjectionWrapper(cell, num_symbols)\n",
    "            output_size = num_symbols\n",
    "\n",
    "        return tf_embedding_attention_decoder(\n",
    "            decoder_inputs,\n",
    "            initial_state,\n",
    "            attention_states,\n",
    "            cell,\n",
    "            num_symbols,\n",
    "            embedding_size,\n",
    "            num_heads=num_heads,\n",
    "            output_size=output_size,\n",
    "            output_projection=output_projection,\n",
    "            feed_previous=feed_previous,\n",
    "            initial_state_attention=initial_state_attention)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_projection_params(size, dtype=None, scope=None):\n",
    "    with variable_scope.variable_scope(scope or \"projection_params\") as scope:\n",
    "        if dtype is not None:\n",
    "            scope.set_dtype(dtype)\n",
    "        else:\n",
    "            dtype = scope.dtype\n",
    "\n",
    "        p_w = tf.get_variable('weights', (size, size), dtype=dtype)\n",
    "        p_b = tf.get_variable('bias', (size,), dtype=dtype)\n",
    "\n",
    "        return p_w, p_b\n",
    "def states_projection(states,\n",
    "                      hidden_size,\n",
    "                      activate_fn=None,\n",
    "                      dtype=None,\n",
    "                      scope=None):\n",
    "    with variable_scope.variable_scope(scope or \"states_projection\") as scope:\n",
    "        if dtype is not None:\n",
    "            scope.set_dtype(dtype)\n",
    "        else:\n",
    "            dtype = scope.dtype\n",
    "\n",
    "        def state_projection(state, _scope=None):\n",
    "            with variable_scope.variable_scope(_scope):\n",
    "                if isinstance(state, LSTMStateTuple):\n",
    "                    c_w, c_b = create_projection_params(hidden_size, dtype=dtype, scope='c')\n",
    "                    c = tf.nn.xw_plus_b(state.c, c_w, c_b, name='c')\n",
    "                    h_w, h_b = create_projection_params(hidden_size, dtype=dtype, scope='h')\n",
    "                    h = tf.nn.xw_plus_b(state.h, h_w, h_b, name='h')\n",
    "                    if activate_fn:\n",
    "                        c = activate_fn(c)\n",
    "                        h = activate_fn(h)\n",
    "                    return LSTMStateTuple(c, h)\n",
    "                else:\n",
    "                    p_w, p_b = create_projection_params(hidden_size, dtype=dtype)\n",
    "                    p = tf.nn.xw_plus_b(state, p_w, p_b)\n",
    "                    if activate_fn:\n",
    "                        p = activate_fn(p)\n",
    "                    return p\n",
    "\n",
    "        if type(states) == tuple:\n",
    "            return tuple(\n",
    "                state_projection(state, 'layer_{}'.format(i)) for i, state in enumerate(states))\n",
    "        else:\n",
    "            return state_projection(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "play.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlbootstrap import Bootstrap\n",
    "from fetch.fetch import Fetcher\n",
    "from process.process import DataProcessor\n",
    "from model.seq import Seq2SeqModel\n",
    "from model.seq_attn import Seq2SeqAttentionModel\n",
    "from model.auto_encoder import AutoEncoderModel\n",
    "\n",
    "models = {\n",
    "    'seq': Seq2SeqModel(),\n",
    "    'seq-attn': Seq2SeqAttentionModel(),\n",
    "    'auto': AutoEncoderModel('auto'),\n",
    "    'auto-attn': AutoEncoderModel('auto-attn')\n",
    "}\n",
    "\n",
    "bootstrap = Bootstrap(\n",
    "    'config.yaml',\n",
    "    fetcher=Fetcher(),\n",
    "    preprocessor=DataProcessor(),\n",
    "    models=models\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'task' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-4a0b335e5d3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mlbootstrap\\bootstrap.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mlbootstrap\\bootstrap.py\u001b[0m in \u001b[0;36m_init_model\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mlbootstrap\\bootstrap.py\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(self, force, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mlbootstrap\\bootstrap.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Text-Generation-using-SOTA\\fetch\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;34m'cornell'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfetch_cornell\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         }\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mfetch_fn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_dataset_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'task' is not defined"
     ]
    }
   ],
   "source": [
    "bootstrap.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
